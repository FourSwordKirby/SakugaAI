\documentclass{article}

% 10-703 Style requirement.
% This will set margins, and make the look-and-feel of the document a research paper
\usepackage[final,nonatbib]{nips_2016}

%%Some semiuseful packages
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage{stmaryrd}
\usepackage{enumerate}
\usepackage{fancyvrb}

\usepackage[english]{babel}
%\usepackage[backend=bibtex]{biblatex}
%\addbibresource{MidtermReport.bib}

\title{15-780 Progress Report}

\author{
  Roger Liu \\
  \texttt{rogerliu@andrew.cmu.edu} \\
  \And
  Sohil Shah \\
  \texttt{sohils@cmu.edu} \\
  \And
  Richard Fan \\
  \texttt{rmfan@andrew.cmu.edu} \\
}
%\date{Due: 28 January 2016}
\begin{document}
\maketitle
\section{Introduction}
Hand-drawn animation is an expensive process and many shortcuts are used to decrease production time and cost. For example, in anime production, the main artists only draw the most important frames, the key frames, and outsource the in-between frames to less-experienced contracted artists. While there are pre-existing techniques for interpolating between frames of videos, they have been known to produce an unnatural "soap-opera effect", something which is not suitable for the very deliberate drawings that are used in traditional animation.

For our project, we attempt to tackle this problem using Deep Learning.

\section{Research and Methods}
Pre-existing research in this topic has produced less than spectacular results so far. 
Both the works of Yahia and Sharma, et al. have attempted to tackle this problem with several deep architectures and configurations, however the in-between frames for both projects are very blurry and noticeably worse than the ground truth image. We hope to remedy this problem through the use of advances made by in the pix2pix paper. Namely, we approach this problem by attempting to 1st predict the edge detected interpolated frame and then filling the results in using the methods in pix2pix

\section{Preliminary Results}
As of this writing, we have gathered and preprocessed x episodes of anime shows Nichijou and Hibike Euphonium to use for our training data. (etc. etc. fill in details and net implementation sohil)


\section{Moving Forward}
So far, we have gathered our data set and brought our architecture up to the same level as the ones presented in the interpolation folder. Moving forward, we will etc. etc.
%\printbibliography

\section{Sources}
To be honest I usually use the biblatex plugin library. For expedience, I'll just include links for now

https://en.wikipedia.org/wiki/Motion_interpolation

https://esc.fnwi.uva.nl/thesis/centraal/files/f1305544686.pdf

http://cs229.stanford.edu/proj2016/report/KorenMendaSharma-ConvolutionalNeuralNetworkForVideoFrameCompression-report.pdf

https://github.com/phillipi/pix2pix
\end{document}
